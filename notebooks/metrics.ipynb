{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading dataset zip file...Loaded (50688).\n"
     ]
    }
   ],
   "source": [
    "import useful\n",
    "data, nyu2_train, nyu2_test  = useful.load_zip('nyu_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DepthDataset, RandomHorizontalFlip, ToTensorTest, ToTensorTrain\n",
    "transformed_training = DepthDataset(data, nyu2_train, transform=ToTensorTrain())\n",
    "transformed_testing = DepthDataset(data, nyu2_test, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[3.3117, 3.3117, 3.3117,  ..., 6.0714, 6.0714, 6.0714],\n",
       "         [3.3117, 3.3117, 3.3117,  ..., 6.0714, 6.0714, 6.0714],\n",
       "         [3.3117, 3.3117, 3.3117,  ..., 6.0714, 6.0714, 6.0714],\n",
       "         ...,\n",
       "         [3.8060, 3.8060, 3.8060,  ..., 9.4444, 9.4444, 9.4444],\n",
       "         [3.8060, 3.8060, 3.8060,  ..., 9.4444, 9.4444, 9.4444],\n",
       "         [3.8060, 3.8060, 3.8060,  ..., 9.4444, 9.4444, 9.4444]]])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "import numpy as np\n",
    "from useful import normalize_depth\n",
    "normalize_depth(transformed_training[7]['depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 3.631082 ,  3.631082 ,  3.6324012, ...,  9.727626 ,  9.727626 ,\n",
       "         9.727626 ],\n",
       "       [ 3.629764 ,  3.631082 ,  3.6324012, ...,  9.727626 ,  9.727626 ,\n",
       "         9.727626 ],\n",
       "       [ 3.628447 ,  3.629764 ,  3.631082 , ...,  9.727626 ,  9.727626 ,\n",
       "         9.737099 ],\n",
       "       ...,\n",
       "       [ 2.7570994,  2.75786  ,  2.7586207, ..., 10.111223 , 10.111223 ,\n",
       "        10.111223 ],\n",
       "       [ 2.75786  ,  2.7586207,  2.7586207, ..., 10.111223 , 10.111223 ,\n",
       "        10.111223 ],\n",
       "       [ 2.7586207,  2.7586207,  2.759382 , ..., 10.111223 , 10.111223 ,\n",
       "        10.111223 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "normalize_depth(torch.clamp(np.array(transformed_testing[5]['depth'], np.float32)/10,10,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[275.2000, 275.2000, 275.2000,  ..., 224.2000, 224.2000, 224.2000],\n",
       "         [275.1000, 275.2000, 275.2000,  ..., 224.2000, 224.2000, 224.2000],\n",
       "         [275.1000, 275.1000, 275.2000,  ..., 224.2000, 224.2000, 224.2000],\n",
       "         ...,\n",
       "         [219.8000, 219.8000, 219.7000,  ..., 208.1000, 208.1000, 208.0000],\n",
       "         [219.7000, 219.7000, 219.7000,  ..., 208.1000, 208.1000, 208.1000],\n",
       "         [219.7000, 219.7000, 219.7000,  ..., 208.1000, 208.1000, 208.1000]]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "transformed_testing[0]['depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from useful import normalize_depth\n",
    "# Load test data\n",
    "def load_test_data():    \n",
    "    print(\"Loading test data...\")\n",
    "    def extract_zip(input_zip):\n",
    "        input_zip=ZipFile(input_zip)\n",
    "        return {name: input_zip.read(name) for name in input_zip.namelist()}\n",
    "    data = extract_zip('nyu_test.zip')\n",
    "    rgb = np.load(BytesIO(data['eigen_test_rgb.npy']))\n",
    "    depth = np.load(BytesIO(data['eigen_test_depth.npy']))\n",
    "    crop = np.load(BytesIO(data['eigen_test_crop.npy']))\n",
    "    print(\"Loaded.\")\n",
    "    return rgb, depth, crop\n",
    "def DepthNorm(x, maxDepth):\n",
    "    return maxDepth / x\n",
    "def compute_errors(gt, pred):\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25  ).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "    log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n",
    "    return a1, a2, a3, abs_rel, rmse, log_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, images, minDepth=10, maxDepth=1000):\n",
    "    if len(images.shape) < 4:\n",
    "        images = np.expand_dims(np.array(images), axis=-1)\n",
    "        images = images.transpose((3, 2, 0, 1))\n",
    "    else:\n",
    "        images = images.transpose((0, 3, 1, 2))\n",
    "    images = torch.from_numpy(images).float()/255\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "    return np.clip(DepthNorm(predictions.cpu().numpy(), maxDepth=1000), minDepth, maxDepth) / maxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(gt, pred):\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25  ).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "    log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n",
    "    return a1, a2, a3, abs_rel, rmse, log_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(model, image, minDepth=10, maxDepth=1000):\n",
    "  image = Image.fromarray(image, 'RGB')\n",
    "  image = image.resize((640, 480))\n",
    "  image = np.expand_dims(np.array(image), axis=-1)\n",
    "  image = image.transpose((3, 2, 0, 1))\n",
    "  image = torch.from_numpy(image).float()/255\n",
    "  #image = image.cuda()\n",
    "  with torch.no_grad():\n",
    "    predictions = model(image)\n",
    "  result = np.clip(DepthNorm(predictions.cpu().numpy(), maxDepth=maxDepth), minDepth, maxDepth) / maxDepth\n",
    "  result = result.reshape(480,640)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_predict(model, rgb[0], minDepth=10, maxDepth=1000)\n",
    "#Image.fromarray((result*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_scores = np.zeros((len(rgb), 6)) # six metrics\n",
    "for i in range(len(rgb)):\n",
    "    result = my_predict(model, rgb[i])\n",
    "    depth_scores[i] = compute_errors(depth[i], result*10)\n",
    "errors = depth_scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        a1,         a2,         a3,        rel,       rmse,     log_10\n    0.7851,     0.9551,     0.9914,     0.1512,     0.5312,     0.0645\n"
     ]
    }
   ],
   "source": [
    "print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rmse', 'log_10'))\n",
    "print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb, depth, crop = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "model = smp.Unet(encoder_name=\"timm-regnety_004\",       \n",
    "                encoder_weights=\"imagenet\",     \n",
    "                in_channels=3,                 \n",
    "                classes=1           \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "#dictt = torch.load('base_timm-regnety_004_model.pt')\n",
    "dictt = torch.load('checkpoint_Regnet.pth')\n",
    "model.load_state_dict(dictt['model_state_dict'] )\n",
    "model.eval()\n",
    "print(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}